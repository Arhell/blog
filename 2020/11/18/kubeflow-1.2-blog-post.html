<p><strong>Kubeflow v1.2 Blog Post</strong></p>

<p><strong>Special Message from the Kubeflow Founders</strong>: Three years (!!) ago, we (Jeremy Lewi, Vish Kannan and David Aronchick) <a href="https://www.youtube.com/watch?v=R3dVF5wWz-g">stood on stage at Kubecon</a> to introduce Kubeflow for the first time. We could not have possibly imagined what would have come about - thousands of GitHub stars, tens of thousands of commits and a community that has built the most flexible and scalable platform for machine learning. And, best of all, it’s not backed by an enormous company that requires you to “upgrade” in order to use it; we gave it all away for free! Here’s to everything you all have done and we could not be more excited about the NEXT three years (and the three years beyond that). Thank you!</p>

<p><strong>Announcing Kubeflow v1.2 release</strong></p>

<p>The Kubeflow Community’s delivery of the Kubeflow 1.2 software release includes ~100 user requested enhancements to improve model building, training, tuning, ML pipelining and serving.  This post includes a Release Highlights Section, which details significant 1.2 features as contributed by the Kubeflow application working groups (WG), SIGs, and ecosystem partners.  The Kubeflow 1.2 <a href="https://github.com/kubeflow/manifests/releases/tag/v1.2-rc.0">changelog</a> provides a quick view of the 1.2 deliveries.</p>

<p>The Release was validated, tested and documented by the developers, and the Release is now being validated, tested and documented by users, cloud providers and commercial support partners on popular platforms i.e. AWS, Azure, GCP, IBM, etc.   The Community is working on a more sustainable approach to owning and maintaining test infrastructure.</p>

<p>For Release 1.2, AWS has built and contributed a shared test-infra, which provides WG owners with enough permissions to identify problems, and test proposed solutions to completion. Currently, most WGs (AutoML, Training-Operators, KFServing, Deployments, Manifests) have already migrated their tests on this solution. As a result, the test-infra blocking time has fallen significantly, which is good for users and contributors.</p>

<p><strong>Getting Involved</strong></p>

<p>The Community continues to grow and we invite new users and contributors to join the Working Groups and Community Meetings. The following provides some helpful links to those looking to get involved with the Kubeflow Community:</p>

<ul>
  <li>Join the <a href="https://app.slack.com/client/T7QLHSH6U/C7REE0EHK/thread/C7REE0EHK-1554222405.030500">Kubeflow Slack channel</a></li>
  <li>Join the <a href="https://groups.google.com/g/kubeflow-discuss">kubeflow-discuss mailing list</a></li>
  <li>Attend a <a href="https://github.com/kubeflow/community">Weekly Community Meeting</a></li>
  <li>Review the Working Group Meeting Notes in Release Highlights Section (as the Notes include great discussions and meeting times)</li>
</ul>

<p>If you have questions and/or run into issues, please leverage the Kubeflow Slack channel and/or submit bugs via <a href="https://github.com/kubeflow">Kubeflow on GitHub</a>.</p>

<p><strong>What’s next</strong></p>

<p>The Community has started discussions on Kubeflow 1.3. Arrikto has agreed to lead the 1.3 Release Management process and the Community will continue to capture input from users and contributors as features are defined, developed and delivered. Onward and upward!</p>

<p>Special thanks to Constantinos Venetsanopoulos (Arrikto), Animesh Singh (IBM), Jiaxin Shan (AWS),  Yao Xiao (AWS), David Aronchick (Azure), Dan Sun (Bloomberg), Andrey Velichkevich (Cisco), Matthew Wicks (Eliiza), Willem Pienaar (Feast), Yuan Gong (Google), James Wu (Google), Jeremy Lewi (Google), Josh Bottum (Arrikto), Rui Vasconcelos (Canonical), Jeff Fogarty (US Bank) , Karl Shriek (AlexanderThamm), and Clive Cox (Seldon) for their help on 1.2 and this post.</p>

<p><strong>Release Highlights Section</strong></p>

<p><strong>Working Group: AutoML / Katib</strong></p>

<p>Working Group Meeting Notes: <a href="https://docs.google.com/document/d/1MChKfzrKAeFRtYqypFbMXL6ZIc_OgijjkvbqmwRV-64/edit#heading=h.yvypq06ot57p">Katib Working Group Meeting Notes</a></p>

<p>Overall benefit: Better model accuracy, Better infrastructure utilization</p>

<p>Overall description: Katib 0.10 with the new v1beta1 API has been released in Kubeflow 1.2. Automated configuration of Hyperparameters to deliver more accuracy models that use less infrastructure, AutoML / Katib simplified the process of finding the optimized set of parameters for your model with Early Stopping techniques. Possibility to orchestrate complex pipeline during Katib Experiment with custom Kubernetes CRD support.</p>

<p>Feature Name: <a href="https://github.com/kubeflow/katib/pull/1344">Early Stopping</a></p>

<p>Feature Description: Save your cluster resources by using Katib Early Stopping techniques. Allow to use the Median Stopping Rule algorithm.</p>

<p>Feature Benefit: You don’t need to modify your training source code to use the feature! Early Stopping can be used with every Katib algorithm.</p>

<p>Feature Name: Support <a href="https://github.com/kubeflow/katib/issues/1214">custom CRD</a> in the <a href="https://github.com/kubeflow/katib/issues/1208">new Trial template</a>.</p>

<p>Feature Description: You are able to follow two simple steps to integrate your custom Kubernetes resource in Katib. Flexible way to send your hyperparameters in the new Trial template design, which is a valid YAML.</p>

<p>Feature Benefit: Define <a href="https://github.com/tektoncd/pipeline">Tekton Pipeline</a> in your Katib experiment. You are able to pass hyperparameters even if your model config is a JSON scikit learn Pipeline.</p>

<p>Feature Name: <a href="https://github.com/kubeflow/katib/issues/1250">Resume Experiments</a></p>

<p>Feature Description: Implementation of the various methods to resume Katib Experiments. Save the Experiment’s Suggestion data in the custom volume. Use Katib config to modify your volume settings.</p>

<p>Feature Benefit: Free your cluster resources after your Experiment is finished.</p>

<p>Feature Name: <a href="https://github.com/kubeflow/katib/pull/1140">Multiple Ways to Extract Metrics</a></p>

<p>Feature Description: You can specify metrics strategies for your Experiment. Katib computes the Experiment objective based on these values. You are able to view detailed metric info for each Trial.</p>

<p>Feature Benefit: Get correct optimisation results when your model produces necessary value at the final training step.</p>

<p><strong>Working Group: KFServing</strong></p>

<p>Working Group Meeting Notes: <a href="https://docs.google.com/document/d/1KZUURwr9MnHXqHA08TFbfVbM8EAJSJjmaMhnvstvi-k/edit">KFServing Working Group Meeting Notes</a></p>

<p>Overall benefit: KFServing enables serverless inferencing on Kubernetes and provides performant, high abstraction interfaces for common machine learning (ML) frameworks like TensorFlow, XGBoost, scikit-learn, PyTorch, and ONNX to solve production model serving use cases.</p>

<p>Overall description:  Kubeflow 1.2 includes KFServing v0.4.1, where the focus has been on enabling KFServing on OpenShift and additionally providing more features, such as adding batcher module as sidecar, Triton inference server renaming and integrations, upgrading Alibi explainer to 0.4.0, updating logger to CloudEvents V1 protocol and allowing customized URL paths on data plane. Additionally, the minimum Istio version is now v1.3.1, and KNative version has been moved to KNative 0.14.3. More details can be found <a href="https://github.com/kubeflow/kfserving/releases/tag/v0.4.0">here</a> and <a href="https://github.com/kubeflow/kfserving/releases/tag/v0.4.1">here</a></p>

<p>Feature Name: Add batcher module as sidecar <a href="https://github.com/kubeflow/kfserving/pull/847">#847</a> <a href="https://github.com/zhangrongguo">@zhangrongguo</a></p>

<p>Feature Description: KFServer Batcher accepts user requests, batch them and then send to the “InferenceService”.  <a href="https://docs.google.com/document/d/14aa5zRtwK1zqcXdpQc9YClTduhtj8vr9waJcm0vR2GQ/edit#">Batcher Feature Description</a></p>

<p>Feature Benefit: Faster response time to inference requests, and Improve infrastructure utilization</p>

<p>Feature Name: Alibi explainer upgrade to 0.4.0 <a href="https://github.com/kubeflow/kfserving/pull/803">#803</a> <a href="https://github.com/cliveseldon">@cliveseldon</a></p>

<p>Feature Description: The enhancements include a KernelSHAP explainer for black-box model SHAP scores and documentation for the LinearityMeasure algorithm.   This delivery includes a new API for explainer and explanation objects, which provide a variety of improvements, but are breaking changes.</p>

<p>Feature Benefit: This delivery improves the ability to understand which features impact model accuracy along with improving operations.</p>

<p>Feature Name/Description : Triton inference server rename and integrations <a href="https://github.com/kubeflow/kfserving/pull/747">#747</a> <a href="https://github.com/deadeyegoodwin">@deadeyegoodwin</a></p>

<p><strong>Working Group: Pipelines</strong></p>

<p>Working Group Meeting Notes: <strong><a href="http://bit.ly/kfp-meeting-notes">http://bit.ly/kfp-meeting-notes</a></strong></p>

<p>Overall benefit: Simplify process of creating a model when you have new data and new code</p>

<p>Overall description: Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on containers.  The Kubeflow Pipelines platform consists of:</p>

<ul>
  <li>A user interface (UI) for managing and tracking experiments, jobs, and runs.</li>
  <li>An engine for scheduling multi-step ML workflows.</li>
  <li>An SDK for defining and manipulating pipelines and components.</li>
  <li>Notebooks for interacting with the system using the SDK.</li>
</ul>

<p>The following are the goals of Kubeflow Pipelines:</p>

<ul>
  <li>End-to-end orchestration: enabling and simplifying the orchestration of machine learning pipelines.</li>
  <li>Easy experimentation: making it easy for you to try numerous ideas and techniques and manage your various trials/experiments.</li>
  <li>Easy re-use: enabling you to re-use components and pipelines to quickly create end-to-end solutions without having to rebuild each time</li>
</ul>

<p>Kubeflow Pipelines is stabilizing over a few patch releases. At the same time, we made a lot of progress at standardizing the <a href="https://github.com/kubeflow/pipelines/blob/master/api/v2alpha1/pipeline_spec.proto">pipeline IR (intermediate representation) </a> which will serve as a unified pipeline definition for different execution engines.</p>

<p>Feature Name: Kubeflow Pipelines with Tekton backend available</p>

<p>Feature Description:  After an extensive effort, we have <a href="https://github.com/kubeflow/kfp-tekton">Kubeflow Pipelines running on Tekton end-to-end and available in open source</a>. Additionally it’s available as default with <a href="https://www.kubeflow.org/docs/ibm/pipelines/">Kubeflow deployment on IBM Cloud</a>, and <a href="https://github.com/kubeflow/kfp-tekton/blob/master/guides/kfp_tekton_install.md#openshift">can be deployed on OpenShift.</a></p>

<p>Feature Benefit: If you are an existing user of Tekton, or are a fan of Tekton, or running OpenShift Pipelines, get Kubeflow Pipelines running on top of it.  More details here <br />
<a href="https://developer.ibm.com/blogs/kubeflow-pipelines-with-tekton-and-watson/">https://developer.ibm.com/blogs/kubeflow-pipelines-with-tekton-and-watson/</a></p>

<p id="gdcalert1"><span style="color: red; font-weight: bold">&gt;&gt;&gt;&gt;&gt;  gd2md-html alert: inline image link here (to images/image1.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br />(<a href="#">Back to top</a>)(<a href="#gdcalert2">Next alert</a>)<br /><span style="color: red; font-weight: bold">&gt;&gt;&gt;&gt;&gt; </span></p>

<p><img src="images/image1.png" alt="alt_text" title="image_tooltip" /></p>

<p>Feature Name: stabilizing Kubeflow Pipelines 1.0.x</p>

<p>Feature Description: We are stabilizing Kubeflow Pipelines over a few patch releases: <a href="https://github.com/kubeflow/pipelines/blob/1.0.4/CHANGELOG.md">Kubeflow Pipelines 1.0.4</a> Changelog ~20 fixes and ~5 minor features.</p>

<p><strong>Working Group: Notebooks</strong></p>

<p>Working Group Meeting Notes: TBD</p>

<p>Overall benefit: Interactive, experimental coding environment for model development</p>

<p>Overall description: Notebooks provide an advanced, interactive coding environment that users and teams can share and leverage kubernetes namespaces for isolation and resource utilization</p>

<p>Feature Name: Affinity/Toleration configs #5237</p>

<p>Feature Description: Adds the ability for Kubeflow administrators to set groups of Affinity/Toleration configs which users can pick from a dropdown.</p>

<p>Feature Benefit: Allows more fine-grained selection of how Notebook pods are scheduled.</p>

<p>Merged code features, artifacts available on 1.2.1 or later</p>

<p>Feature Name: Refactor Notebook</p>

<p>Feature Description: The details of the refactoring are defined in these deliveries:</p>

<ul>
  <li>Common code between the different python backends,  <a href="https://github.com/kubeflow/kubeflow/pull/5164">#5164</a></li>
  <li>Create an Angular Library with common frontend code, <a href="https://github.com/kubeflow/kubeflow/pull/5252">#5252</a></li>
  <li>Refactor the JWA backend to utilize common code, <a href="https://github.com/kubeflow/kubeflow/pull/5316">#5316</a></li>
  <li>Initialize the Jupyter web app frontend in crud-web-apps, <a href="https://github.com/kubeflow/kubeflow/pull/5332">#5332</a></li>
</ul>

<p>Feature Benefit : Refactoring will enable an easier future integration with other web apps - Tensorboard, volume manager.</p>

<p>Feature Name: Stop and Restart Notebooks while maintaining state, <a href="https://github.com/kubeflow/kubeflow/issues/4857">#4857</a></p>

<p>Feature Description: Implementation of a “shut down server” button in the central dashboard that scales the stateful set for the server down to zero and a “start server” button that scales it back up again.   Note - this feature will be supported in 1.2.1 or later.</p>

<p>Feature Benefit: Save work, save infrastructure resources</p>

<p><strong>Working Group: Training-Operators</strong></p>

<p>Working Group Meeting Notes: TBD</p>

<p>Overall benefit: Faster model development using operators that simplify distributed computing</p>

<p>Feature Name: The Training Operator contributors provided the following fixes and improvements in Kubeflow 1.2:</p>

<ul>
  <li>Add mxnet-operator to aws kfdefs (<a href="https://github.com/kubeflow/manifests/pull/1327">#1327</a>, <a href="https://github.com/Jeffwan">@Jeffwan</a>)</li>
  <li>Update mxnet-operator manifest to v1 (<a href="https://github.com/kubeflow/manifests/pull/1326">#1326</a>, <a href="https://github.com/Jeffwan">@Jeffwan</a>)</li>
  <li>Correct XGBoostJob CRD group name and add singular name (<a href="https://github.com/kubeflow/manifests/pull/1313">#1313</a>, <a href="https://github.com/terrytangyuan">@terrytangyuan</a>)</li>
  <li>Fix XGBoost Operator manifest issue (<a href="https://github.com/kubeflow/manifests/pull/1463">#1463</a>, <a href="https://github.com/Jeffwan">@Jeffwan</a>)</li>
</ul>

<p><strong>SIG: Model Management</strong></p>

<p>Overall benefit: The ability to find model versions and their subcomponents including metadata</p>

<p>SIG Meeting Notes: <a href="https://docs.google.com/document/d/1R2r1z5O4USpn3BW29-4blCoPueB3p2LAjvdKa97fT7U/edit?usp=sharing">Model Management SIG Meeting Notes</a></p>

<p>Overall description:</p>

<p>The SIG was initiated to define and develop a Kubeflow solution for model management, which will make it easier to organize and find models and their artifacts.   In addition, several contributors are submitting proposals on how to define data types for ML model and data, with the goal of driving wider metadata standards, and interoperability of models between ML platforms, clouds, and frameworks.  The proposals are working to define an ontology for model and data types and tooling to search and organize that metadata.</p>

<p>Proposals from Kubeflow Pipelines contributors, the Model Management SIG, Seldon and a MLSpec from David Aronchick (Azure) are under discussion.   Please find links to those proposals below:</p>

<ul>
  <li><a href="https://docs.google.com/document/d/1YhnsMK_ktaeUKLS2XShocq4u2bcUCUcztT9rioRssOM/edit#">ML Data in Kubeflow Pipelines</a></li>
  <li><a href="https://github.com/mlspec/schemas">ML Spec from David Aronchick</a></li>
  <li><a href="https://docs.google.com/document/d/1R2r1z5O4USpn3BW29-4blCoPueB3p2LAjvdKa97fT7U/edit">Model Management Proposal from Karl Schriek, SIG Tech Lead</a></li>
  <li><a href="https://docs.google.com/document/d/1yhSWRAwyB8AnxPz0FtJACHrMnj1m0g2uRiQC6GaT4UE/edit?usp=sharing">Seldon’s Proposal for Initial Metadata Types</a></li>
</ul>

<p><strong>EcoSystem: Seldon</strong></p>

<p>Overall benefit: Deploy, Scale, Update models built with Kubeflow.</p>

<p>Overall description: Seldon handles scaling of production machine learning models and provides advanced machine learning capabilities out of the box including Advanced Metrics, Request Logging, Explainers, Outlier Detectors, A/B Tests, and Canaries.</p>

<p>Kubeflow 1.2 comes with Seldon’s 1.4 release. This release of Seldon adds further capabilities for model deployment and inferencing including the addition of batch and streaming interfaces to a deployed model. It also allows for fine grained control of how a deployed model interfaces with Kubernetes with the addition of KEDA and Pod Disruption Budget options. Finally, it begins a process of compatibility with KFServing by allowing the usage of the V2 Dataplane supported by Seldon, KFServing and NVIDIA Triton.</p>

<p>Version: 1.4.0</p>

<p>Feature Name:  Stream and Batch support</p>

<p>Feature Description: Streaming support for native Kafka integration. Batch prediction support from and to cloud storage.</p>

<p>Feature Benefit: Allows Seldon users to interact with their models via RPC, Streaming or Batch as needed.</p>

<p>Feature Name: Extended kubernetes control via KEDA and <a href="https://docs.seldon.io/projects/seldon-core/en/v1.4.0_a/graph/disruption-budgets.html">PDBs</a></p>

<p>Feature Description: Allows fine grained control of deployed models via autoscaling with KEDA metrics and addition of pod disruption budgets.</p>

<p>Feature Benefit: Manage models at scale in a production cluster.</p>

<p>Feature Name: <a href="https://docs.seldon.io/projects/seldon-core/en/v1.4.0_a/graph/protocols.html#v2-kfserving-protocol">Alpha V2 Dataplane</a>.</p>

<p>Feature Description: Run custom python models using an updated python server along with support for the V2 Dataplane (NVIDIA Triton, KFServing, Seldon)</p>

<p>Feature Benefit: Utilize a standard powerful protocol that is supported cross project.</p>

<p><strong>EcoSystem: Kale</strong></p>

<p>Overall benefit: Kubeflow Workflow tool that simplifies ML pipeline building and versioning directly from a Notebook or IDE i.e. VSCode</p>

<p>Kale GitHub repo: https://github.com/kubeflow-kale/kale</p>

<p>Kale Tutorials:  <a href="https://www.arrikto.com/tutorials/">https://www.arrikto.com/tutorials/</a></p>

<p>Overall description: Kale lets you deploy Jupyter Notebooks that run on your laptop or on the cloud to Kubeflow Pipelines, without requiring any of the Kubeflow SDK boilerplate. You can define pipelines just by annotating Notebook’s code cells and clicking a deployment button in the Jupyter UI. Kale will take care of converting the Notebook to a valid Kubeflow Pipelines deployment, taking care of resolving data dependencies and managing the pipeline’s lifecycle</p>

<p>Feature Name: Dog Breed Classification example</p>

<p>Feature Description: Tutorial for simplified pipeline to build a model for Image Classification</p>

<p>Feature Benefit: Faster understanding of ML workflows to deliver models with hyperparameter tuning</p>

<p>Feature Name: Katib integration with Kale</p>

<p>Feature Description: Automated hyperparameter tuning and reproducible katib trials using pipelines</p>

<p>Feature Benefit: Better model accuracy and easy reproducibility and debugging</p>

<p>Feature Name: Pipeline Step Caching for Katib Trials using Kale’s integration with Rok</p>

<p>Feature Description: Kale recognizes when a pipeline step has been run before and fetches complete results from Rok and inserts into pipeline processing</p>

<p>Feature Benefit: Faster hyperparameter tuning, reduced infrastructure utilization</p>

<p><strong>EcoSystem: Feast</strong></p>

<p>Overall benefit: Feast allows teams to register, ingest, serve, and monitor machine learning features in production.</p>

<p>Working Group Meeting Notes: <a href="https://tinyurl.com/kf-feast-sig">https://tinyurl.com/kf-feast-sig</a></p>

<p>Overall description: The latest release of Feast was a concerted effort by the Feast community to make Feast available in more environments than Google Cloud. We’ve removed all hard couplings to managed services and made it possible to run Feast both on AWS and locally.</p>

<p>Version: Feast 0.8</p>

<p>Feature Name: Support for AWS</p>

<p>Feature Description: Feast 0.8 now comes with support for deployment on AWS, with native support for job management on EMR, and support for both S3 and Kinesis as data sources.</p>

<p>Feature Benefit: Finally makes it possible for Kubeflow users on AWS to run Feast</p>

<p>Feature Name: Batch-only ingestion</p>

<p>Feature Description: Allows teams to ingest data into stores without passing the data through a stream.</p>

<p>Feature Benefit: Allows for a more performant ingestion compared to the stream-first approach.</p>

<p>Feature Name: Local-only mode</p>

<p>Feature Description: Makes it possible to run Feast without any external infrastructure, using only Docker Compose or Minikube</p>

<p>Feature Benefit: Lowers the barrier to entry for new users, and makes it easier to test and develop Feast</p>

<p><strong>EcoSystem: On-Prem SIG</strong></p>

<p>Description: The on-prem SIG was officially created during this release with the intent to develop best practices for Kubeflow deployment in on-prem installations. With the new release, the SIG has also secured testing infrastructure in order to provide a well-tested reference architecture.</p>

<p>SIG Meeting Notes: <a href="https://bit.ly/2LyTh14">https://bit.ly/2LyTh14</a></p>

<p>Slack channel: https://kubeflow.slack.com/archives/C01C9NPD15H</p>

<p><strong>Platform: AWS</strong></p>

<p>Description: Better reliability, better testing coverage by enabling E2E tests for Kubeflow AWS deployment, <a href="https://www.kubeflow.org/docs/aws/notebook-server/">better Kubeflow notebook user experience</a>.</p>

<p><strong>Platform: IBM</strong></p>

<p>Description: Pipelines and Security have been the key focus for Kubeflow on IBM Cloud for this release. On the Pipelines side, <a href="https://www.kubeflow.org/docs/ibm/pipelines/">Kubeflow Pipelines with Tekton is available for deployment on IBM Cloud Kubernetes Service</a> and is included by default with Kubeflow deployment on IBM Cloud. On the security side, <a href="https://www.kubeflow.org/docs/ibm/deploy/install-kubeflow/#multi-user-auth-enabled">we have enabled integration with IBM Cloud AppId</a> as an authentication provider instead of Dex. When using AppID, it delegates the identity provider to IBM Cloud with builtin identity providers (Cloud Directory, SAML, social log-in with Google or Facebook etc.) or custom providers. Additionally for securing the <a href="https://www.kubeflow.org/docs/ibm/deploy/authentication/">Kubeflow authentication with HTTPS we have provided integration instructions</a> using the <a href="https://cloud.ibm.com/docs/containers?topic=containers-loadbalancer-about">IBM Cloud Network Load Balancer</a>.</p>

<p><strong>Platform: GCP</strong></p>

<p>Description: Better UX and reliability for installation and upgrade. Upgrade Cloud Config Connector in management cluster to latest.</p>

<p><strong>Platform: Azure</strong></p>

<p>Description:</p>

<p><strong>Platform: OpenShift</strong></p>

<p>Description: Our focus for this release was to create the OpenShift stack that can install Kubeflow components on OpenShift 4.x . We architected the stack so users can pick and choose components they would like to install by adding or removing kustomizeConfig entries in the kfdef. Components currently supported are istio, single user pipeline, Jupyter notebooks with a custom Tensorflow notebook image, profile controller with custom image, Katib, pytorch and Tensorflow job operatorss and Seldon. You can install Kubeflow 1.2 on Openshift from the Open Data Hub community operator in OpenShift Catalog using the OpenShift kfdef.</p>

<p><strong>Platform: MicroK8s</strong></p>

<p>Description: Kubeflow is a built-in add-on to MicroK8s, and now includes Istio v1.5 as default.</p>

<p><strong>Platform: MiniKF</strong></p>

<p>Description: MiniKF is currently testing with Kubeflow 1.2 and will provide an updated MiniKF version based after validation testing and documentation has completed.    Please find more information on MiniKF here: <a href="https://www.arrikto.com/get-started/">https://www.arrikto.com/get-started/</a> . You can also find tutorials that will guide you through end-to-end data science examples here: <a href="https://www.arrikto.com/tutorials">https://www.arrikto.com/tutorials</a></p>
